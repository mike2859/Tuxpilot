using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using Tuxpilot.Core.Interfaces.Services;

namespace Tuxpilot.Infrastructure.Services;


/// <summary>
/// Service d'assistant IA utilisant Ollama en local
/// </summary>
public class ServiceOllama : IServiceAssistantIA
{
    private readonly HttpClient _httpClient;
    private readonly string _modele = "mistral"; // Mod√®le par d√©faut
    private readonly string _urlOllama = "http://localhost:11434"; // Port par d√©faut d'Ollama
    
    public ServiceOllama()
    {
        _httpClient = new HttpClient
        {
            Timeout = TimeSpan.FromMinutes(5) // Timeout long pour les r√©ponses LLM
        };
    }
    
   public async Task<string> DemanderAsync(string question)
{
    try
    {
        // üÜï Nouveau prompt avec d√©tection d'actions
        var systemPrompt = @"Tu es un assistant Linux expert qui aide les utilisateurs francophones.

IMPORTANT - D√âTECTION D'ACTIONS :
Si l'utilisateur demande d'INSTALLER, SUPPRIMER, EX√âCUTER une commande, ou FAIRE quelque chose, tu dois r√©pondre au format JSON suivant :

{
  ""type"": ""action"",
  ""action"": ""install"" ou ""remove"" ou ""execute"",
  ""command"": ""la commande compl√®te"",
  ""package"": ""nom du paquet si applicable"",
  ""explanation"": ""Explication courte de ce qui sera fait"",
  ""needsSudo"": true ou false
}

Exemples de requ√™tes ACTION :
- ""Installe VLC"" ‚Üí JSON avec action: install, command: ""dnf install vlc""
- ""Comment installer VLC ?"" ‚Üí JSON avec action: install
- ""Supprime Firefox"" ‚Üí JSON avec action: remove
- ""Red√©marre Apache"" ‚Üí JSON avec action: execute, command: ""systemctl restart httpd""

Exemples de requ√™tes NORMALES (pas JSON) :
- ""C'est quoi VLC ?"" ‚Üí R√©ponse texte normale
- ""Comment fonctionne dnf ?"" ‚Üí R√©ponse texte normale
- ""Quelle est la diff√©rence entre..."" ‚Üí R√©ponse texte normale

Si la requ√™te est une ACTION, r√©ponds UNIQUEMENT avec le JSON, rien d'autre.
Si c'est une question normale, r√©ponds en texte comme d'habitude.";

        var fullPrompt = $"{systemPrompt}\n\nQuestion de l'utilisateur : {question}";
        
        var requestBody = new
        {
            model = _modele,
            prompt = fullPrompt,
            stream = false
        };
        
        var response = await _httpClient.PostAsJsonAsync(
            $"{_urlOllama}/api/generate",
            requestBody
        );
        
        response.EnsureSuccessStatusCode();
        
        var result = await response.Content.ReadFromJsonAsync<OllamaResponse>();
        
        return result?.Response ?? "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse.";
    }
    catch (HttpRequestException ex)
    {
        return $"‚ùå Erreur de connexion √† Ollama : {ex.Message}\n\nAssurez-vous qu'Ollama est d√©marr√© avec : ollama serve";
    }
    catch (Exception ex)
    {
        return $"‚ùå Erreur : {ex.Message}";
    }
}
    
    public async Task DemanderAvecStreamingAsync(string question, Action<string> onTokenReceived)
    {
        try
        {
            var requestBody = new
            {
                model = _modele,
                prompt = $"Tu es un assistant Linux expert qui aide les utilisateurs francophones. R√©ponds de mani√®re claire et concise.\n\nQuestion: {question}",
                stream = true
            };
            
            var content = new StringContent(
                JsonSerializer.Serialize(requestBody),
                Encoding.UTF8,
                "application/json"
            );
            
            var request = new HttpRequestMessage(HttpMethod.Post, $"{_urlOllama}/api/generate")
            {
                Content = content
            };
            
            using var response = await _httpClient.SendAsync(request, HttpCompletionOption.ResponseHeadersRead);
            response.EnsureSuccessStatusCode();
            
            using var stream = await response.Content.ReadAsStreamAsync();
            using var reader = new StreamReader(stream);
            
            while (!reader.EndOfStream)
            {
                var line = await reader.ReadLineAsync();
                if (string.IsNullOrWhiteSpace(line))
                    continue;
                
                try
                {
                    var json = JsonSerializer.Deserialize<OllamaStreamResponse>(line);
                    if (json?.Response != null)
                    {
                        onTokenReceived(json.Response);
                    }
                    
                    // Si c'est le dernier message
                    if (json?.Done == true)
                        break;
                }
                catch (JsonException)
                {
                    // Ignorer les lignes mal form√©es
                    continue;
                }
            }
        }
        catch (HttpRequestException ex)
        {
            onTokenReceived($"\n\n‚ùå Erreur de connexion √† Ollama : {ex.Message}\n\nAssurez-vous qu'Ollama est d√©marr√© avec : ollama serve");
        }
        catch (Exception ex)
        {
            onTokenReceived($"\n\n‚ùå Erreur : {ex.Message}");
        }
    }

    public async Task<string> AnalyserSystemeAsync(
        double pourcentageRam, 
        double pourcentageCpu, 
        double pourcentageDisque,
        int nombreMisesAJour)
    {
        try
        {
            // Construire un prompt contextualis√©
            var prompt = $@"Tu es un assistant Linux expert. Analyse cet √©tat syst√®me et donne 1 ou 2 suggestions COURTES et ACTIONNABLES en fran√ßais.

√âtat actuel :
- RAM utilis√©e : {pourcentageRam:F1}%
- CPU utilis√© : {pourcentageCpu:F1}%
- Disque utilis√© : {pourcentageDisque:F1}%
- Mises √† jour en attente : {nombreMisesAJour}

R√®gles :
- Si RAM > 80% : sugg√®re de voir les processus gourmands
- Si Disque > 85% : sugg√®re le nettoyage
- Si Mises √† jour > 0 : sugg√®re de les installer
- Sinon : dis que tout va bien

R√©ponds en 2-3 phrases MAX, avec des emojis. Soit concret et actionnable.";

            var requestBody = new
            {
                model = _modele,
                prompt = prompt,
                stream = false
            };
        
            var response = await _httpClient.PostAsJsonAsync(
                $"{_urlOllama}/api/generate",
                requestBody
            );
        
            response.EnsureSuccessStatusCode();
        
            var result = await response.Content.ReadFromJsonAsync<OllamaResponse>();
        
            return result?.Response ?? "‚úÖ Votre syst√®me fonctionne bien !";
        }
        catch (Exception ex)
        {
            return $"‚ùå Impossible d'analyser le syst√®me : {ex.Message}";
        }
    }

    // Classes pour d√©s√©rialiser les r√©ponses Ollama
    private class OllamaResponse
    {
        public string? Response { get; set; }
    }
    
    private class OllamaStreamResponse
    {
        public string? Response { get; set; }
        public bool Done { get; set; }
    }
}